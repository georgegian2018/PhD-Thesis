%*******************************************************************************
%****************************** Sixth Chapter **********************************
%*******************************************************************************

\chapter{Concluding remarks}

In this thesis we introduced the history of single cell RNA-sequencing (Chapter \ref{ch:intro}), technically evaluated the methods (Chapter \ref{ch:power}), and investigated how to use these technologies to study cellular development and differentiation (Chapters \ref{ch:zebrafish}-\ref{ch:spatial}).

The field of single cell RNA sequencing is starting to mature. In the beginning it was unclear how representative the measurements were, and it was not known how technical noise affects the measurements. The most striking result of our initial assessment of the scRNA-seq protocols was that the measurements are quantitaive, and can reflect different levels of molecular copy numbers with high precision (Chapter \ref{ch:power}).

As a consequence, we were comfortable studying systems of continuous, changing, quantitative expression levels in cells. Time courses are demanding experiments, and would in many cases require artificial \textit{in vitro} systems. Here we have looked at \textit{ex vivo} data both in the context of blood development and immune response. These experiments would not be possible without learning from snapshots.

Since the conceptual introduction of the notion of \name{pseudotime} to single cell transcriptomics studies \cite{Trapnell2014-cn}, attempts to learn underlying trajectories from single snapshots have gotten extremely popular. This is not dissimilar from the introduction of shotgun DNA sequencing, where small fragments of DNA are sequenced, then reconstructed computationally to whole chromosomes. One way to think about this approach is as a ``shotgun time course''. With the broad prior expectation that gene expression follows smooth functions during cellular development, we used Gaussian process models to study these data. Both for ordering cells by Gaussian process latent variable models and classifying them with mixture models, and to analyse individual genes by variations on Gaussian process regression.

With this strategy, we discovered the underlying patterns of gene expression as hematopoietic progenitor cells specialize to thrombocytes. Whereas this system have classically been studied in terms of discrete cell populations, we found a continuum of differentiation. Intermediate cells between progenitors and thrombocytes identifid by our analysis were verified phenotypically and by replication experiments. The differentiation continuum was related to decrease in transcription and translation programs, and a steady increase in key functionallt important thrombocyte genes (Chapter \ref{ch:zebrafish}).

We were also able to study cellular decision making in the immune system in the same way. Our analysis strategy let us learn a timeline of events during the CD4+ immune response to malaria: Cells 1) get activated, 2) clonally expand, 3) enter a highly proliferative state, 4) specialize towards sub-cell type, 5) stop proliferating and terminally differentiate. These events could be related to real time, and the models we used allowed us to identify genes related to these events (Chapter \ref{ch:malaria}).

The main focus of this thesis has been regarding the problem of studying gene expression over continuous development and inferring time from snapshots. Prior work on continuous trends of RNA expression during cell development or differentiation was limited, which led us to consider non-parametric regression methods. This has enabled us to find very general temporal patterns of gene expression.

These general models do however come with a downside. While we can identify genes which does anything we deem ``interesting", followup questions, such as ``when is it activated?", ``how quickly does it go down?", ``when does it peak?", \textit{etc}, are not possible to answer in other ways than by heuristic downstream analysis.

Questions such as the ones listed above are typical, along with a number of other standard comments from wet lab researchers and other people unfamiliar with non-parametric analysis. The nature of these questions could provide insight into the expected behavior of temporal expression functions. Recent studies have proposed sigmoidal functions \cite{Campbell2016-bd} or impulse functions \cite{Sander2016-by} as definitive of biologically meaningful behavior. In our work, Chapter \ref{ch:zebrafish} and \cite{Eckersley-Maslin2016-cz} are consistent with the idea of sigmoidal functions. However, in Chapter \ref{ch:malaria} a substantial fraction of interesting and important genes follow transient expression, related to the proliferative status of the cells, consistent with impulse-like functions.

In those cases however, time was learned from the data using the latent variable model. This might bias the resolution and uncertainty of the \name{pseudotime} for the cells, since the GPLVM only considers a single length scale for all genes. In our re-analysis of a high resolution whole-transcriptome time course, top interesting genes follow functions which are extremely hard to pin down a parametric form for, with remarkably low observation noise (Figure \ref{fig:ss8}B, e.g. \textit{cog2}, \textit{gsn}, or \textit{hunk}). Results from clustering time courses as in Chapter \ref{ch:zebrafish}, or from inspection of significantly time dependent genes might allow us to restrict the general temporal trends.

When applying the latent variable model to the frog development data, the learned \name{pseudotime} and real time are highly rank correlated. But we can appreciate variable speed of \name{pseudotime} compared to real time, reflecting more fast-acting transcriptional changes in the early part of development (Figure \ref{fig:real_vs_pseudo}). In ancient greek there are two words for time: ``chronos'', for quantitaive time, and ``kairos'' for \textit{qualitative} time. From the perspective of the biological system in the frog embryos, real time (\textit{chronos}) passes faster in the later part of the time course. ``Shotgun time course'' experiments might miss important events on short time scales due to the difficulty sampling enough cells to notice a signal for these.

The value of a large number of known measurement to perform Gaussian process analysis on was further demonstrated in Chapter \ref{ch:spatial}, and we find clearer signals in this spatial setting than in pseudotime settings. A fantastic technological development would be the ability to parallelize time course experiments the way these spatial experiments are. Even in an \textit{in vitro} setting, this could be valuable to verify interesting expression patterns discovered from snapshots.

Beyond the ability to answer questions about the properties of trends, another reason to move to parametric models is the growth of data. Most of the work discussed in this thesis have used older technologies with medium throughput, but the newer methods have orders of magnitude larger scale (Chapter \ref{ch:intro}). While we show in Chapter \ref{ch:spatial} that we can make highly efficient scalable methods in this modelling framework, some underlying concepts for Gaussian process models might not be appropriate for massive data, especially when \name{pseudotime} time as missing data.

Gaussian process models are highly data efficient, and perform well with relatively few observations. With larger data, simpler models could potentially be used. It will likely not be feasilble to use latent variable models as the data grows substantially. In latent variable models each observation has one or more parameters associated with them which need to be fitted. Learning latent functions which in stead summarize the data will be more powerful. Such functions should be able to take the transcriptome readout of a cell, and predict what part of trajectory it came from. Lacking a ground truth reference for time, this could be done with autoencoding strategies: train a model which predicts time from transcriptome (encoder), jointly with a model which predict the transcriptome from time (decoder).

Gaussian process regression is suitible for the latter part, allowing extremely flexible non-linear functions from time to expression. It is however known that gaussian processes perform poorly with large numbers of predictors, and so the encoding model would need a different strategy. In image analysis deep neural networks are a popular choice for these problems, but it might be the case that simpler parametric functions suffice.
