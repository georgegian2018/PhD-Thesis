%*******************************************************************************
%****************************** Sixth Chapter **********************************
%*******************************************************************************

\chapter{Concluding remarks}

In this thesis we introduced the history of single cell RNA-sequencing (Chapter \ref{ch:intro}), technically evaluated the methods (Chapter \ref{ch:power}), and investigated how to use these technologies to study cellular development and differentiation (Chapters \ref{ch:zebrafish}-\ref{ch:spatial}).

The field of single cell RNA sequencing is starting to mature. In the beginning it was unclear how representative the measurements were and it was not known how technical noise affects the measurements. The most striking result of our initial assessment of the scRNA-seq protocols was that the measurements are quantitative, and can reflect different levels of expression are captured with high precision (Chapter \ref{ch:power}).

As a consequence, we were comfortable studying systems of continuous and changing expression levels in cells. Time course analyses are demanding experiments and would in many cases require artificial \textit{in vitro} systems. To avoid this, we have shown how snapshots that capture different stages of a time series in a single experiment can be very informative. We have looked at \textit{ex vivo} data both in the context of blood development and immune responses.

Since the conceptual introduction of the notion of \name{pseudotime} to single cell transcriptomics studies \cite{Trapnell2014-cn}, attempts to learn underlying trajectories from single snapshots have become extremely popular. \name{Pseudotime} snapshots can be compared to the introduction of shotgun DNA sequencing, where small fragments of DNA are sequenced, then reconstructed computationally to whole chromosomes. One way to think about this approach is as a ``shotgun time course''. Given the prior expectation that gene expression follows smooth functions during cellular development, we used Gaussian process models to study these data. We have used Gaussian process latent variable models for ordering cells and classifying them with mixture models, and to analyse individual genes, variations on Gaussian process regression were employed.

With this strategy, we discovered the underlying patterns of gene expression as hematopoietic progenitor cells specialize to thrombocytes. Whereas this system has classically been studied in terms of discrete cell populations, we found a continuum of differentiation. Intermediate cell types between progenitors and thrombocytes identifid by our analysis were verified phenotypically and by replication experiments. The differentiation continuum correlated with decreases in general transcription and translation programs and a steady increase in the expression of functionally important thrombocyte genes (Chapter \ref{ch:zebrafish}).

We were able to study cellular decision making in the immune system in the same way. Our analysis strategy allowed us to establish a timeline of events during the CD4+ T-cell immune response to malaria: Cells 1) get activated, 2) clonally expand, 3) enter a highly proliferative state, 4) specialize towards sub-cell type, 5) stop proliferating and undergo terminal differentiation. These events could be related to real time (days of infection), and the models we used allowed us to identify genes related to these events (Chapter \ref{ch:malaria}), in particular relating cell proliferation status to cell fate choice.

The focus of this thesis is the development of methods to allow the analysis of gene expression over a continuous timeline, reflecting cell differentiation or development. Prior work on continuous trends of RNA expression during cell development or differentiation was limited, which led us to consider non-parametric regression methods. This has enabled us to find very general temporal patterns of gene expression.

While we can identify genes which we deem ``interesting" these general models do, however, come with a downside. Followup questions, such as ``when is it activated?", ``how quickly does it go down?", ``when does it peak?", \textit{etc}, are not possible to answer in other ways than by heuristic downstream analysis of individual genes.

Questions such as the ones listed above could provide insight into the expected behavior of temporal expression functions, but only for individual genes. Recent studies have proposed sigmoidal functions \cite{Campbell2016-bd} or impulse functions \cite{Sander2016-by} as definitive of biologically meaningful behavior. Our results  (chapter \ref{ch:zebrafish} and \citet{Eckersley-Maslin2016-cz} are consistent with the idea of sigmoidal functions. However, in Chapter \ref{ch:malaria} a substantial fraction of interesting and important genes follow transient expression, related to the proliferative status of the cells, consistent with impulse-like functions.

In our studies however, time was learned from the data using the latent variable model. This might bias the resolution and uncertainty of the \name{pseudotime} for the cells, since the GPLVM only considers a single length scale for all genes. In our re-analysis of a high resolution whole-transcriptome time course, the most interesting genes follow functions which are extremely hard to model in a parametric form. Although the functions were complex, curves were very reproducible with little observation noise (Figure \ref{fig:ss8}B, e.g. \textit{cog2}, \textit{gsn}, or \textit{hunk}). Results from clustering time courses as in Chapter \ref{ch:zebrafish} or from inspection of significantly time dependent genes might allow us to identify parametric forms for the general temporal trends.

When applying the latent variable model to the frog development data, the learned \name{pseudotime} and real time are highly rank correlated. But we can appreciate variable speed of \name{pseudotime} compared to real time, reflecting more fast-acting transcriptional changes in the early part of development (Figure \ref{fig:real_vs_pseudo}). In ancient greek there are two words for time: ``chronos'' for quantitaive time and ``kairos'' for \textit{qualitative} time. From the perspective of the biological system in the frog embryos, real time (\textit{chronos}) passes faster in the later part of the time course. ``Shotgun time course'' experiments might miss important events on short time scales due to the difficulty of sampling enough cells to notice a signal for these.

The value of a large number of known measurement to perform Gaussian process analysis on was further demonstrated by our analysis of spatial expression patterns in Chapter \ref{ch:spatial}. We find clearer signals in this spatial setting than in pseudotime settings. A fantastic technological development would be the ability to parallelize time course experiments to match the data density we see in spatial experiments. Even in an \textit{in vitro} setting, this could be valuable for verifying interesting expression patterns that were discovered from snapshots.

Beyond the ability to answer questions about the properties of trends, another reason to move to parametric models is the growth of data. Most of the work discussed in this thesis is based on experiments using older technologies with medium throughput. Newer methods are able to generate data with orders of magnitude larger scale (Chapter \ref{ch:intro}). While we show in Chapter \ref{ch:spatial} that we can design highly efficient scalable methods in this modelling framework, some underlying concepts for Gaussian process models might not be appropriate for massive data, especially with \name{pseudotime} time as missing data.

Gaussian process models are highly data efficient and perform well with relatively few observations. With larger data, simpler models could potentially be used. It unlikely to be feasilble to use latent variable models as the data grows substantially. In latent variable models each observation has one or more parameters associated with them, which need to be fitted. Learning latent functions which summarize the data instead of latent variables for each data point will be more powerful. Such functions should be able to take the transcriptome of a cell, and predict what part of the trajectory it came from. Lacking a ground truth reference for time, this could be done with autoencoding strategies: train a model which predicts time from transcriptome (encoder), jointly with a model which predict the transcriptome from time (decoder).

Gaussian process regression is suitible for the latter part, allowing extremely flexible non-linear functions from time to expression. It is however known that Gaussian processes perform poorly with large numbers of predictors, and so the encoding model would need a different strategy. In image analysis deep neural networks are a popular choice for these problems, but it might be the case that simpler parametric functions suffice.

In conclusion, we have harnessed Gaussian processes to design analysi techniques that have given novel biological insights from complex data sets and will be applicable in many other setting.
